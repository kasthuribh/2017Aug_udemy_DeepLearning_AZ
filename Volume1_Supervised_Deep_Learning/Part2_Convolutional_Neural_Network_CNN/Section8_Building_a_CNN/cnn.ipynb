{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Building the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#folder structure\n",
    "#dataset\n",
    "    #test_set\n",
    "        #cats\n",
    "        #dogs\n",
    "    #train_set\n",
    "        #cats\n",
    "        #dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialising the CNN\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 1 : Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filters - no of feature detectors (filters,convolution kernel), for each filter, feature map will be created\n",
    "# kernel_size - no of rows and columns in the feature detector\n",
    "# strides - strides of the convolution along the width and height\n",
    "# input_shape - specify the expected format of our input images\n",
    "#  (rows, cols, no_of_channels), channels=3 since coloured images\n",
    "#activation - to remove linearity\n",
    "classifier.add(Convolution2D(filters=32, kernel_size=(3,3) , input_shape=(64,64,3), activation='relu' ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pool_size - windows size, most of the time we use 2,2\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2),))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adding another convolution layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#adding another convolution layer\n",
    "# no need to define a input_layer since this is not the very first layer (Keras knows the input size)\n",
    "#here the input is the pooled feature maps from the above layer\n",
    "# in general filter size is dobled (64), but in this case 32 is enough\n",
    "classifier.add(Convolution2D(filters=32, kernel_size=(3,3) , activation='relu' ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we are addding the fully connected hidden layer\n",
    "#units - no of nodes in the hidden layer, should not be too small, power of 2 is generally used\n",
    "classifier.add(Dense(units=128, activation='relu' ))\n",
    "#output layer\n",
    "classifier.add(Dense(units=1, activation='sigmoid' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compiling the CNN\n",
    "# if there are more than two classes use categorical cross entropy for the loss function\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy' , metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Fitting the CNN to the images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# too few images -> overfitting, not able to find generalized patterns\n",
    "# we need to have many images or a trick - > image augmentation\n",
    "# image Augmentation - amount of training images are augmented (rotate, flip,...) to minimize overfitting\n",
    "\n",
    "# get the code from the Keras documentation , preprocesing -> image preprocessing, \n",
    "#                               Example of using .flow_from_directory(directory):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import  ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        'dataset/training_set',\n",
    "        target_size=(64, 64),  #dimension expected by CNN\n",
    "        batch_size=32,\n",
    "        class_mode='binary')  #binary or more than 2\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "        'dataset/test_set',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "8000/8000 [==============================] - 1240s - loss: 0.4273 - acc: 0.7965 - val_loss: 0.5504 - val_acc: 0.7838\n",
      "Epoch 2/25\n",
      "8000/8000 [==============================] - 1183s - loss: 0.2301 - acc: 0.9038 - val_loss: 0.7316 - val_acc: 0.7835\n",
      "Epoch 3/25\n",
      "8000/8000 [==============================] - 1182s - loss: 0.1440 - acc: 0.9432 - val_loss: 0.8900 - val_acc: 0.7738\n",
      "Epoch 4/25\n",
      "8000/8000 [==============================] - 1181s - loss: 0.1025 - acc: 0.9617 - val_loss: 1.1165 - val_acc: 0.7750\n",
      "Epoch 5/25\n",
      "8000/8000 [==============================] - 1182s - loss: 0.0811 - acc: 0.9703 - val_loss: 1.1671 - val_acc: 0.7804\n",
      "Epoch 6/25\n",
      "8000/8000 [==============================] - 1181s - loss: 0.0653 - acc: 0.9767 - val_loss: 1.2448 - val_acc: 0.7822\n",
      "Epoch 7/25\n",
      "8000/8000 [==============================] - 1182s - loss: 0.0564 - acc: 0.9801 - val_loss: 1.3159 - val_acc: 0.7753\n",
      "Epoch 8/25\n",
      "8000/8000 [==============================] - 1182s - loss: 0.0501 - acc: 0.9825 - val_loss: 1.3437 - val_acc: 0.7802\n",
      "Epoch 9/25\n",
      "8000/8000 [==============================] - 1182s - loss: 0.0436 - acc: 0.9852 - val_loss: 1.3728 - val_acc: 0.7825\n",
      "Epoch 10/25\n",
      "8000/8000 [==============================] - 1181s - loss: 0.0401 - acc: 0.9864 - val_loss: 1.4971 - val_acc: 0.7633\n",
      "Epoch 11/25\n",
      "8000/8000 [==============================] - 1181s - loss: 0.0365 - acc: 0.9881 - val_loss: 1.5114 - val_acc: 0.7668\n",
      "Epoch 12/25\n",
      "8000/8000 [==============================] - 1181s - loss: 0.0340 - acc: 0.9887 - val_loss: 1.4904 - val_acc: 0.7783\n",
      "Epoch 13/25\n",
      "8000/8000 [==============================] - 1182s - loss: 0.0309 - acc: 0.9897 - val_loss: 1.5127 - val_acc: 0.7720\n",
      "Epoch 14/25\n",
      "8000/8000 [==============================] - 1183s - loss: 0.0278 - acc: 0.9909 - val_loss: 1.6113 - val_acc: 0.7715\n",
      "Epoch 15/25\n",
      "8000/8000 [==============================] - 1181s - loss: 0.0265 - acc: 0.9913 - val_loss: 1.6125 - val_acc: 0.7766\n",
      "Epoch 16/25\n",
      "8000/8000 [==============================] - 1181s - loss: 0.0253 - acc: 0.9918 - val_loss: 1.6194 - val_acc: 0.7764\n",
      "Epoch 17/25\n",
      "8000/8000 [==============================] - 1181s - loss: 0.0240 - acc: 0.9923 - val_loss: 1.6100 - val_acc: 0.7812\n",
      "Epoch 18/25\n",
      "8000/8000 [==============================] - 1182s - loss: 0.0231 - acc: 0.9925 - val_loss: 1.6863 - val_acc: 0.7702\n",
      "Epoch 19/25\n",
      "8000/8000 [==============================] - 1183s - loss: 0.0212 - acc: 0.9930 - val_loss: 1.7704 - val_acc: 0.7570\n",
      "Epoch 20/25\n",
      "8000/8000 [==============================] - 1182s - loss: 0.0202 - acc: 0.9935 - val_loss: 1.7881 - val_acc: 0.7603\n",
      "Epoch 21/25\n",
      "8000/8000 [==============================] - 1181s - loss: 0.0192 - acc: 0.9939 - val_loss: 1.7878 - val_acc: 0.7692\n",
      "Epoch 22/25\n",
      "8000/8000 [==============================] - 1182s - loss: 0.0190 - acc: 0.9941 - val_loss: 1.7467 - val_acc: 0.7659\n",
      "Epoch 23/25\n",
      "8000/8000 [==============================] - 1182s - loss: 0.0178 - acc: 0.9942 - val_loss: 1.7554 - val_acc: 0.7700\n",
      "Epoch 24/25\n",
      "8000/8000 [==============================] - 1182s - loss: 0.0172 - acc: 0.9945 - val_loss: 1.7190 - val_acc: 0.7747\n",
      "Epoch 25/25\n",
      "8000/8000 [==============================] - 1182s - loss: 0.0167 - acc: 0.9947 - val_loss: 1.7664 - val_acc: 0.7649\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x8941a10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit_generator(\n",
    "        training_set,\n",
    "        steps_per_epoch=8000, #all the observations passes through the CNN. thus no of images in the training set\n",
    "        epochs=25,\n",
    "        validation_data=test_set,\n",
    "        validation_steps=1000 # no of images in the test set\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
