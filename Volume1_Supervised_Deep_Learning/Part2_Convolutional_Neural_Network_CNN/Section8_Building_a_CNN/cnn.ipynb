{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Building the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#folder structure\n",
    "#dataset\n",
    "    #test_set\n",
    "        #cats\n",
    "        #dogs\n",
    "    #train_set\n",
    "        #cats\n",
    "        #dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialising the CNN\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 1 : Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filters - no of feature detectors (filters,convolution kernel), for each filter, feature map will be created\n",
    "# kernel_size - no of rows and columns in the feature detector\n",
    "# strides - strides of the convolution along the width and height\n",
    "# input_shape - specify the expected format of our input images\n",
    "#  (rows, cols, no_of_channels), channels=3 since coloured images\n",
    "#activation - to remove linearity\n",
    "classifier.add(Convolution2D(filters=32, kernel_size=(3,3) , input_shape=(64,64,3), activation='relu' ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pool_size - windows size, most of the time we use 2,2\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adding another convolution layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#adding another convolution layer\n",
    "# no need to define a input_layer since this is not the very first layer (Keras knows the input size)\n",
    "#here the input is the pooled feature maps from the above layer\n",
    "# in general filter size is dobled (64), but in this case 32 is enough\n",
    "classifier.add(Convolution2D(filters=32, kernel_size=(3,3) , activation='relu' ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we are addding the fully connected hidden layer\n",
    "#units - no of nodes in the hidden layer, should not be too small, power of 2 is generally used\n",
    "classifier.add(Dense(units=128, activation='relu' ))\n",
    "#output layer\n",
    "classifier.add(Dense(units=1, activation='sigmoid' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compiling the CNN\n",
    "# if there are more than two classes use categorical cross entropy for the loss function\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy' , metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Fitting the CNN to the images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# too few images -> overfitting, not able to find generalized patterns\n",
    "# we need to have many images or a trick - > image augmentation\n",
    "# image Augmentation - amount of training images are augmented (rotate, flip,...) to minimize overfitting\n",
    "\n",
    "# get the code from the Keras documentation , preprocesing -> image preprocessing, \n",
    "#                               Example of using .flow_from_directory(directory):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import  ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        'dataset/training_set',\n",
    "        target_size=(64, 64),  #dimension expected by CNN\n",
    "        batch_size=32,\n",
    "        class_mode='binary')  #binary or more than 2\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "        'dataset/test_set',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "8000/8000 [==============================] - 1345s - loss: 0.3891 - acc: 0.8140 - val_loss: 0.5388 - val_acc: 0.8034\n",
      "Epoch 2/25\n",
      "8000/8000 [==============================] - 1333s - loss: 0.1747 - acc: 0.9287 - val_loss: 0.6864 - val_acc: 0.8103\n",
      "Epoch 3/25\n",
      "8000/8000 [==============================] - 1331s - loss: 0.1031 - acc: 0.9608 - val_loss: 0.8171 - val_acc: 0.8027\n",
      "Epoch 4/25\n",
      "8000/8000 [==============================] - 1334s - loss: 0.0759 - acc: 0.9719 - val_loss: 0.8924 - val_acc: 0.8038\n",
      "Epoch 5/25\n",
      "8000/8000 [==============================] - 1331s - loss: 0.0589 - acc: 0.9788 - val_loss: 1.0637 - val_acc: 0.8150\n",
      "Epoch 6/25\n",
      "8000/8000 [==============================] - 1330s - loss: 0.0495 - acc: 0.9827 - val_loss: 1.0499 - val_acc: 0.8021\n",
      "Epoch 7/25\n",
      "8000/8000 [==============================] - 1331s - loss: 0.0429 - acc: 0.9850 - val_loss: 1.1242 - val_acc: 0.8021\n",
      "Epoch 8/25\n",
      "8000/8000 [==============================] - 1331s - loss: 0.0377 - acc: 0.9873 - val_loss: 1.1056 - val_acc: 0.8108\n",
      "Epoch 9/25\n",
      "8000/8000 [==============================] - 1330s - loss: 0.0343 - acc: 0.9883 - val_loss: 1.1119 - val_acc: 0.8100\n",
      "Epoch 10/25\n",
      "8000/8000 [==============================] - 1332s - loss: 0.0307 - acc: 0.9895 - val_loss: 1.1568 - val_acc: 0.8042\n",
      "Epoch 11/25\n",
      "8000/8000 [==============================] - 1332s - loss: 0.0284 - acc: 0.9903 - val_loss: 1.1771 - val_acc: 0.8123\n",
      "Epoch 12/25\n",
      "8000/8000 [==============================] - 1333s - loss: 0.0260 - acc: 0.9913 - val_loss: 1.1788 - val_acc: 0.8101\n",
      "Epoch 13/25\n",
      "8000/8000 [==============================] - 1337s - loss: 0.0240 - acc: 0.9921 - val_loss: 1.1684 - val_acc: 0.8059\n",
      "Epoch 14/25\n",
      "8000/8000 [==============================] - 1340s - loss: 0.0235 - acc: 0.9920 - val_loss: 1.1644 - val_acc: 0.8089\n",
      "Epoch 15/25\n",
      "8000/8000 [==============================] - 1331s - loss: 0.0215 - acc: 0.9930 - val_loss: 1.2250 - val_acc: 0.8122\n",
      "Epoch 16/25\n",
      "8000/8000 [==============================] - 1332s - loss: 0.0200 - acc: 0.9934 - val_loss: 1.2744 - val_acc: 0.8131\n",
      "Epoch 17/25\n",
      "8000/8000 [==============================] - 1331s - loss: 0.0202 - acc: 0.9933 - val_loss: 1.3108 - val_acc: 0.8106\n",
      "Epoch 18/25\n",
      "8000/8000 [==============================] - 1331s - loss: 0.0190 - acc: 0.9939 - val_loss: 1.1996 - val_acc: 0.8071\n",
      "Epoch 19/25\n",
      "8000/8000 [==============================] - 1333s - loss: 0.0186 - acc: 0.9940 - val_loss: 1.2269 - val_acc: 0.8118\n",
      "Epoch 20/25\n",
      "8000/8000 [==============================] - 1352s - loss: 0.0179 - acc: 0.9940 - val_loss: 1.2853 - val_acc: 0.8074\n",
      "Epoch 21/25\n",
      "8000/8000 [==============================] - 1409s - loss: 0.0164 - acc: 0.9947 - val_loss: 1.3329 - val_acc: 0.7992\n",
      "Epoch 22/25\n",
      "8000/8000 [==============================] - 1404s - loss: 0.0164 - acc: 0.9948 - val_loss: 1.3440 - val_acc: 0.8094\n",
      "Epoch 23/25\n",
      "8000/8000 [==============================] - 1403s - loss: 0.0168 - acc: 0.9945 - val_loss: 1.3861 - val_acc: 0.8140\n",
      "Epoch 24/25\n",
      "8000/8000 [==============================] - 1403s - loss: 0.0156 - acc: 0.9950 - val_loss: 1.5006 - val_acc: 0.8099\n",
      "Epoch 25/25\n",
      "8000/8000 [==============================] - 1402s - loss: 0.0156 - acc: 0.9949 - val_loss: 1.4437 - val_acc: 0.8073\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x93dae90>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit_generator(\n",
    "        training_set,\n",
    "        steps_per_epoch=8000, #all the observations passes through the CNN. thus no of images in the training set\n",
    "        epochs=25,\n",
    "        validation_data=test_set,\n",
    "        validation_steps=2000 # no of images in the test set\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making new predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#making new predictions (single prediction)\n",
    "import numpy as np\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read image\n",
    "file_name='''/home/cloudera/courses/2017Aug_udemy_DeepLearningAZ/Deep_Learning_AZ/Volume1_Supervised_Deep_Learning/Part2_Convolutional_Neural_Network_CNN/Section8_Building_a_CNN/dataset/single_prediction/cat_or_dog_2.jpg'''\n",
    "test_image=image.load_img(file_name, target_size=(64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAIAAAAlC+aJAAAd+UlEQVR4nJ15d3Rdx3nnzNx+37uv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\ndWJU/PGmkloJRxRJCvw/Sdv0n5qgvFUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=64x64 at 0x8FD7F90>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image to array to get 3D array as our input layer\n",
    "test_image=image.img_to_array(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 3)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prediction inputs take inputs in batches. \n",
    "#Thus there shoud be 04 dimensions to the input, including dimension to store batch information\n",
    "#input expects this new dimension as the first index, axis=0\n",
    "test_image=np.expand_dims(test_image,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 64, 64, 3)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result=classifier.predict(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.]], dtype=float32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cats': 0, 'dogs': 1}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# does 1 corresponds to cat or dog?\n",
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dog\n"
     ]
    }
   ],
   "source": [
    "if result[0][0]==1:\n",
    "    print 'Dog'\n",
    "else:\n",
    "    print 'Cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
